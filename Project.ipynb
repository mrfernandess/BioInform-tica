{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1855ec1a-a081-4cdc-a5fb-9b04ac33e286",
   "metadata": {},
   "source": [
    "## Initial Setup and Library Imports\n",
    "\n",
    "In this cell, we import the necessary libraries for data analysis, visualization, machine learning, and deep learning. Additionally, we define the following:\n",
    "\n",
    "- **Standard Amino Acid List**: `AMINO_ACIDS` contains the 20 amino acids encoded by DNA.\n",
    "- **Mapping Dictionaries**:\n",
    "  - `AA_TO_INT`: converts amino acids into integers.\n",
    "  - `STRUCTURE_TO_INT_SST3`: maps simplified secondary structures (3 categories).\n",
    "  - `STRUCTURE_TO_INT_SST8`: maps detailed secondary structures (8 categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa5e018b-f4ac-4394-955f-e321b18f4241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import label_binarize\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Standard amino acids\n",
    "AMINO_ACIDS = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "AA_TO_INT = {aa: i for i, aa in enumerate(AMINO_ACIDS)}\n",
    "\n",
    "# Maps for labels\n",
    "STRUCTURE_TO_INT_SST3 = {'H': 0, 'E': 1, 'C': 2}\n",
    "STRUCTURE_TO_INT_SST8 = {'H': 0, 'G': 1, 'I': 2, 'E': 3, 'B': 4, 'T': 5, 'S': 6, 'C': 7}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1840b0-67f8-4bec-9eb0-e0c40e1eb1e5",
   "metadata": {},
   "source": [
    "## Utility Functions for Data Preprocessing  \n",
    "\n",
    "These functions perform essential preprocessing steps for sequence and label data:  \n",
    "\n",
    "1. **`normalize_features(features)`**:  \n",
    "   - Normalizes features to a range of 0 to 1.  \n",
    "\n",
    "2. **`one_hot_encode_sequence(sequence)`**:  \n",
    "   - Converts an amino acid sequence into a one-hot encoded matrix.  \n",
    "\n",
    "3. **`encode_labels(labels, structure_map)`**:  \n",
    "   - Maps labels to integers based on a provided structure map (e.g., `sst3` or `sst8`).  \n",
    "\n",
    "4. **`pad_sequences(sequences, max_len, padding_value=0.0)`**:  \n",
    "   - Pads sequences to a uniform length (`max_len`) with a specified padding value.  \n",
    "\n",
    "5. **`pad_labels(labels, max_len, padding_value=-1)`**:  \n",
    "   - Pads label sequences to `max_len` using a default padding value of `-1`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceed018-8397-44fb-9d98-bfff10960ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(features):\n",
    "    return (features - np.min(features)) / (np.max(features) - np.min(features))\n",
    "\n",
    "def one_hot_encode_sequence(sequence):\n",
    "    one_hot = np.zeros((len(sequence), len(AMINO_ACIDS)), dtype=np.float32)\n",
    "    for i, aa in enumerate(sequence):\n",
    "        if aa in AA_TO_INT:\n",
    "            one_hot[i, AA_TO_INT[aa]] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "def encode_labels(labels, structure_map):\n",
    "    return np.array([structure_map[label] for label in labels])\n",
    "\n",
    "def pad_sequences(sequences, max_len, padding_value=0.0):\n",
    "    padded = np.full((len(sequences), max_len, len(AMINO_ACIDS)), padding_value, dtype=np.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded[i, :len(seq), :] = seq\n",
    "    return padded\n",
    "\n",
    "def pad_labels(labels, max_len, padding_value=-1):\n",
    "    padded = np.full((len(labels), max_len), padding_value, dtype=np.int32)\n",
    "    for i, label in enumerate(labels):\n",
    "        padded[i, :len(label)] = label\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c964629-4d99-440e-8715-7bd1ecac1972",
   "metadata": {},
   "source": [
    "## Data Preprocessing Function  \n",
    "\n",
    "The `preprocess_data` function processes a dataset for model input.  \n",
    "\n",
    "- **Inputs**:  \n",
    "  - `df`: DataFrame containing sequences (`seq`) and structure labels.  \n",
    "  - `structure_type`: Specifies the structure mapping (`sst3` or `sst8`).  \n",
    "\n",
    "- **Steps**:  \n",
    "  1. Determines the appropriate structure map (`sst3` or `sst8`).  \n",
    "  2. Normalizes and one-hot encodes sequences.  \n",
    "  3. Encodes labels using the selected structure map.  \n",
    "  4. Pads sequences and labels to the length of the longest sequence.  \n",
    "\n",
    "- **Outputs**:  \n",
    "  - `sequences_padded`: Padded, normalized, one-hot encoded sequences.  \n",
    "  - `labels_padded`: Padded labels mapped to integers.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2faee2-adbd-4b7b-aa0e-0fa8385362d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, structure_type='sst3'):\n",
    "    structure_map = STRUCTURE_TO_INT_SST3 if structure_type == 'sst3' else STRUCTURE_TO_INT_SST8\n",
    "    sequences = [normalize_features(one_hot_encode_sequence(seq)) for seq in df['seq']]\n",
    "    labels = [encode_labels(label, structure_map) for label in df[structure_type]]\n",
    "    max_len = max(len(seq) for seq in df['seq'])\n",
    "    sequences_padded = pad_sequences(sequences, max_len)\n",
    "    labels_padded = pad_labels(labels, max_len)\n",
    "    return sequences_padded, labels_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0012b43e-e12a-4045-902d-436b634b946d",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "The `load_and_preprocess_data` function loads and preprocesses training and testing datasets:  \n",
    "\n",
    "- **Inputs**: Paths to the training (`train_path`) and testing (`test_path`) datasets, and the secondary structure type (`sst3` or `sst8`).  \n",
    "- **Steps**:  \n",
    "  - Filters sequences to include only standard amino acids.  \n",
    "  - Preprocesses data based on the selected structure type.  \n",
    "- **Outputs**: Processed features and labels for training (`X_train`, `y_train`) and testing (`X_test`, `y_test`).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29690905-d10d-441d-8b8e-caf12136771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(train_path, test_path, structure_type='sst3'):\n",
    "    \n",
    "    # load data and filter out non-standard amino acids\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    train_df['is_standard'] = train_df['seq'].apply(lambda x: all(aa in AMINO_ACIDS for aa in x))\n",
    "    train_df = train_df[train_df['is_standard']]\n",
    "    X_train, y_train = preprocess_data(train_df, structure_type=structure_type)\n",
    "    \n",
    "    # load test data and filter out non-standard amino acids\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    test_df['is_standard'] = test_df['seq'].apply(lambda x: all(aa in AMINO_ACIDS for aa in x))\n",
    "    test_df = test_df[test_df['is_standard']]\n",
    "    X_test, y_test = preprocess_data(test_df, structure_type=structure_type)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9e3f64-ca87-4be6-bc3b-86dcb06e794b",
   "metadata": {},
   "source": [
    "## Function to Save Experiment Results  \n",
    "\n",
    "Saves the results of experiments (such as accuracy, hyperparameters) to a CSV file. If the file doesn't exist, it creates a new one and adds a header.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f68409c-66cb-4548-a84d-c511a68c0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(results, filename='experiment_results.csv'):\n",
    "    file_exists = False\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            file_exists = True\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "    with open(filename, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        if not file_exists:\n",
    "             writer.writerow([\"algorithm_type\", \"num_layers\", \"learning_rate\", \"batch_size\", \"hidden_size\", \"dropout_rate\", \"train_accuracy\", \"val_accuracy\"])\n",
    "        \n",
    "        writer.writerow(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb0ce8-ba7c-4772-9bc3-6fd0251f304e",
   "metadata": {},
   "source": [
    "## RNNModel  \n",
    "\n",
    "Defines a customizable RNN-based model (`RNN`, `GRU`, or `LSTM`).  \n",
    "\n",
    "- **Inputs**: Sequence data.  \n",
    "- **Output**: Predictions for each timestep.  \n",
    "- **Key Features**:  \n",
    "  - Supports multiple RNN types.  \n",
    "  - Includes dropout to prevent overfitting.  \n",
    "  - Fully connected layer for output mapping.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e971f-2dbc-48f0-a56a-dcc05b311739",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_rate, rnn_type, device='cuda'):\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # choose the type of RNN\n",
    "        if rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        elif rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Dropout layer to prevent overfitting (applied after the RNN layer)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        if isinstance(self.rnn, nn.LSTM):\n",
    "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "            out, _ = self.rnn(x, (h0, c0))\n",
    "        else:\n",
    "            out, _ = self.rnn(x, h0)\n",
    "\n",
    "        out = self.dropout(out) \n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef56fdb0-f0ae-4fc7-b2e3-731b69d4826a",
   "metadata": {},
   "source": [
    "## Calculate Metrics  \n",
    "Calculates evaluation metrics from the confusion matrix.  \n",
    "\n",
    "- **Inputs**:  \n",
    "  - `y_true`: True labels.  \n",
    "  - `y_pred`: Predicted labels.  \n",
    "  - `num_classes`: Number of classes in the dataset.  \n",
    "\n",
    "- **Outputs**:  \n",
    "  - **Sensitivities**: True positive rate for each class.  \n",
    "  - **Specificities**: True negative rate for each class.  \n",
    "  - **Confusion Matrix (cm)**: A matrix showing true positives, false positives, true negatives, and false negatives.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6202790-f84f-4c36-a768-a1165c541ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, num_classes):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(num_classes))\n",
    "    sensitivities = cm.diagonal() / cm.sum(axis=1)\n",
    "    specificities = (cm.sum(axis=0) - cm.diagonal()) / (cm.sum(axis=0) + cm.sum(axis=1) - cm.diagonal())\n",
    "    return sensitivities, specificities, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32ea082-2af9-4147-be51-94438392d894",
   "metadata": {},
   "source": [
    "## Function: Train and Evaluate Model\n",
    "\n",
    "This function trains and evaluates an RNN model and displays results in graphs and metrics.\n",
    "\n",
    "### Inputs:\n",
    "- **X_train**: Training data (features).\n",
    "- **y_train**: Training labels.\n",
    "- **X_test**: Test data (features).\n",
    "- **y_test**: Test labels.\n",
    "- **input_size**: Size of input features.\n",
    "- **output_size**: Number of output classes.\n",
    "- **hidden_size**: Hidden layer size (default: 128).\n",
    "- **num_epochs**: Number of training epochs (default: 20).\n",
    "- **batch_size**: Batch size for training (default: 64).\n",
    "- **lr**: Learning rate (default: 1e-3).\n",
    "- **num_layers**: Number of RNN layers (default: 2).\n",
    "- **dropout_rate**: Dropout rate (default: 0.3).\n",
    "- **rnn_type**: Type of RNN ('GRU', 'LSTM', 'RNN').\n",
    "\n",
    "### Outputs:\n",
    "- **Loss and Accuracy Graphs**: Shows training and test loss/accuracy.\n",
    "- **Class-wise Sensitivity and Specificity**: Performance metrics for each class.\n",
    "- **F1 Score**: Weighted F1 score.\n",
    "- **Confusion Matrix**: Displays the confusion matrix.\n",
    "- **CSV File**: Saves results to a CSV file.\n",
    "\n",
    "### Function Steps:\n",
    "1. **Preprocessing**: Converts data to PyTorch tensors.\n",
    "2. **Training**: Trains the model for the specified epochs.\n",
    "3. **Evaluation**: Evaluates the model on test data.\n",
    "4. **Metric Calculation**: Calculates sensitivity, specificity, and confusion matrix.\n",
    "5. **Visualization**: Plots loss and accuracy graphs.\n",
    "6. **Results**: Saves training and test accuracies to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309eb54c-264a-42a6-b750-e2f3d701db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, y_train, X_test, y_test, input_size, output_size, hidden_size = 128, num_epochs=20, batch_size=64, lr=1e-3, num_layers = 2 , dropout_rate = 0.3, rnn_type='GRU'):\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = RNNModel(input_size, hidden_size, output_size, num_layers, dropout_rate, rnn_type, device=device).to(device)  \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_train, total_train = 0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, output_size), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, dim=2)\n",
    "            mask = (labels != -1)\n",
    "            correct_train += (predicted[mask] == labels[mask]).sum().item()\n",
    "            total_train += mask.sum().item()\n",
    "\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "        train_accuracies.append(100 * correct_train / total_train)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss, total, correct = 0, 0, 0\n",
    "        y_true, y_pred  = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, output_size), labels.view(-1))\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, dim=2)\n",
    "                mask = (labels != -1)\n",
    "                total += mask.sum().item()\n",
    "                correct += (predicted[mask] == labels[mask]).sum().item()\n",
    "                y_true.extend(labels[mask].cpu().numpy())\n",
    "                y_pred.extend(predicted[mask].cpu().numpy())\n",
    "                \n",
    "        test_losses.append(test_loss / len(test_loader))\n",
    "        test_accuracies.append(100 * correct / total)\n",
    "        \n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        sensitivities, specificities, cm = calculate_metrics(y_true, y_pred, output_size)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}, Test Accuracy: {test_accuracies[-1]:.2f}%\")\n",
    "\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    plt.figure(figsize=(12, 7))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, test_losses, label=\"Test Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss (Overfitting Analysis)\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\n",
    "    plt.plot(epochs, test_accuracies, label=\"Test Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Training vs Test Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nClass-wise Metrics:\")\n",
    "    print(f\"{'Class':<5} {'Sensitivity':<12} {'Specificity':<12}\")\n",
    "    for i, (sens, spec) in enumerate(zip(sensitivities, specificities)):\n",
    "        print(f\"{i}       {sens:.4f}        {spec:.4f}\")\n",
    "\n",
    "\n",
    "    print(\"F1 Score:\", f1)\n",
    "    \n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['H', 'E', 'C'])\n",
    "    disp.plot(cmap=\"Blues\")\n",
    "    plt.show()\n",
    "    \n",
    "    avg_train_accuracy = sum(train_accuracies) / len(train_accuracies)\n",
    "    avg_val_accuracy = sum(test_accuracies) / len(test_accuracies)\n",
    "    \n",
    "    results = [rnn_type, num_layers, lr, batch_size, hidden_size, dropout_rate, avg_train_accuracy, avg_val_accuracy]\n",
    "    save_results_to_csv(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2a89e6-4073-4a9e-a3e2-f54c836ecdc6",
   "metadata": {},
   "source": [
    "## Random Search for Hyperparameter Tuning\n",
    "\n",
    "### Inputs:\n",
    "- **num_experiments**: Number of experiments.\n",
    "- **X_train, y_train, X_test, y_test**: Training and test data.\n",
    "- **input_size, output_size**: Model input and output sizes.\n",
    "\n",
    "### Hyperparameter Space:\n",
    "- Learning rate, hidden size, dropout rate, layers, batch size, RNN type.\n",
    "\n",
    "### Process:\n",
    "1. Randomly select hyperparameters.\n",
    "2. Train and evaluate the model.\n",
    "3. Repeat for **num_experiments**.\n",
    "\n",
    "### Output:\n",
    "- Prints parameters and evaluation results for each experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba2550-b8d8-4532-80c2-cf9d58357b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(num_experiments, X_train, y_train, X_test, y_test, input_size, output_size):\n",
    "    \n",
    "    lr_values = [1e-3, 1e-4, 1e-5, 5e-4, 1e-2] \n",
    "    hidden_sizes = [64, 128, 256, 512]  \n",
    "    dropout_rates = [0.1, 0.3, 0.5, 0.7]  \n",
    "    num_layers_values = [1, 2, 3, 4]  \n",
    "    batch_sizes = [16, 32, 64, 128] \n",
    "    rnn_types = ['GRU', 'LSTM', 'RNN']  \n",
    "    \n",
    "    for _ in range(num_experiments):\n",
    "    \n",
    "        lr = random.choice(lr_values)\n",
    "        hidden_size = random.choice(hidden_sizes)\n",
    "        dropout_rate = random.choice(dropout_rates)\n",
    "        num_layers = random.choice(num_layers_values)\n",
    "        batch_size = random.choice(batch_sizes)\n",
    "        rnn_type = random.choice(rnn_types)\n",
    "        \n",
    "        print(f\"\\nStarting experiment with parameters: lr={lr}, hidden_size={hidden_size}, dropout_rate={dropout_rate}, num_layers={num_layers}, batch_size={batch_size}, rnn_type={rnn_type}\")\n",
    "        \n",
    "    \n",
    "        train_and_evaluate(X_train, y_train, X_test, y_test, input_size,output_size, hidden_size,\n",
    "                           num_epochs=20, batch_size=batch_size, lr=lr, \n",
    "                           num_layers=num_layers, dropout_rate=dropout_rate, rnn_type=rnn_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de16f90b-630c-48f3-80b8-1569ad10da30",
   "metadata": {},
   "source": [
    "# Main Function\n",
    "\n",
    "The `main` function is responsible for loading and preprocessing the training and test datasets and running a random search for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211041bd-e7dc-4ed2-ab25-979ec567fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_path = \"Dataset\\\\training_secondary_structure_train.csv\"\n",
    "    test_path = \"Dataset\\\\test_secondary_structure_cb513.csv\"\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_and_preprocess_data(train_path, test_path)\n",
    "    \n",
    "    input_size = len(AMINO_ACIDS)\n",
    "    output_size = len(STRUCTURE_TO_INT_SST3)\n",
    "    \n",
    "    #train_and_evaluate(X_train, y_train, X_test, y_test, input_size, output_size)\n",
    "    \n",
    "    random_search(20, X_train, y_train, X_test, y_test, input_size, output_size)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
